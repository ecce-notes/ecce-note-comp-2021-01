
%\textbf{\emph{David L.}}

The Electron Ion Collider (EIC) is the next generation precision nuclear physics accelerator complex that is being constructed in the United States. The EIC is expected to start producing data in the early 2030's, and is unique as it will collide high energy polarized electrons with polarized protons and a wide range of nuclei. As such, it will introduce new paradigms in large scale nuclear physics experiments. Expected luminosities at the EIC will reach upwards of $10^{34}$ cm$^{-2}$s$^{-1}$; consequently, there will be an extremely large data sample to process. Recent efforts from modern collider physics experiments have shown the benefits of near real-time analysis~\cite{Benson_2015,Aaij_2019}. Therefore, there is a strong desire to develop software and computing infrastructure that reliably and quickly processes data for analysis. 



As the EIC will start taking data in nearly a decade, there are a number of new paradigms that have the opportunity to be explored in this software R\&D phase. An example of such a new paradigm is the use of Artificial Intelligence (AI) and machine learning (ML). The EIC has the unique opportunity to be one of the first large-scale facilities to systematically incorporate and employ AI, starting from the detector design and R\&D phases. The relevance of AI for the EIC has been highlighted among the \textit{Opportunities for Computing} of the EIC Yellow Report \cite{YellowReport}. AI potentially permeates all aspects of this Computing Plan; in fact, it already plays a significant role in the design and R\&D phases of the EIC~\cite{cisbani2020ai}~\cite{AI4EIC2021}.

This document presents a proposed computing plan for the EIC Comprehensive Chromodynamics Experiment (ECCE) detector at the EIC~\cite{YellowReport}. This includes estimates of the rates from the detector, the pipeline for processing and storing the data, and how the collaboration members will access the data. Software systems for monitoring, calibration, reconstruction, and analysis are discussed. Estimates of the computing and storage requirements are included. AI detector optimization techniques are also discussed as this is expected to be a large part of the computing effort over the next few years. While we attempt to include some forward thinking plans in this regard, we necessarily do need to rely on past experience with other large experiments such as sPHENIX~\cite{sphenix_computing_plan_2019} and LHCb~\cite{CAMPORAPEREZ2016280} to serve as guides.






